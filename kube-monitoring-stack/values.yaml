grafana:
  adminPassword: password!@#$
  ingress:
    enabled: true
    ingressClassName: traefik
    hosts:
      - grafana.local
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
      - name: Prometheus
        type: prometheus
        url: http://k8s-monitoring-prometheus-server.monitoring:80      
        isDefault: true
        editable: true
      - name: Mimir
        type: prometheus
        url: http://k8s-monitoring-mimir-nginx.monitoring.svc:80/prometheus
        editable: true
      - name: Loki
        type: loki
        url: http://k8s-monitoring-loki-distributed-gateway
        editable: true
      - name: Tempo
        type: tempo
        url: http://k8s-monitoring-k8s-monitoring-tempo:3100
        editable: true

  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
      - name: 'default'
        orgId: 1
        folder: 'default'
        type: file
        disableDeletion: true
        editable: true
        options:
          path: /var/lib/grafana/dashboards/standard
  dashboards:
    default:
      k8s_global:
        gnetId: 15757
        revision: 1
        datasource: Mimir
      k8s_nodes:
        gnetId: 15759
        revision: 1
        datasource: Mimir  
      k8s_namespaces:
        gnetId: 15758
        revision: 1
        datasource: Mimir
      k8s_pods:
        gnetId: 15760
        revision: 1
        datasource: Mimir
prometheus:
  server:
    ingress:
      enabled: true
      ingressClassName: traefik
      hosts:
        - prometheus.local
    remoteWrite:
      - url: http://k8s-monitoring-mimir-nginx.monitoring.svc:80/api/v1/push
    global:
      external_labels:
        cluster: kw-local

mimir-distributed:
  minio:
    consoleIngress:
      enabled: true
      ingressClassName: traefik
      hosts:
        - minio.local
    buckets: 
      - name: loki-store
        policy: none
        purge: false
        versioning: false
        objectlocking: false
      - name: tempo-store
        policy: none
        purge: false
        versioning: false
        objectlocking: false
      - name: mimir-tsdb
        policy: none
        purge: false
      - name: mimir-ruler
        policy: none
        purge: false
      - name: enterprise-metrics-tsdb
        policy: none
        purge: false
      - name: enterprise-metrics-admin
        policy: none
        purge: false
      - name: enterprise-metrics-ruler
        policy: none
        purge: false

loki-distributed:
  loki:
    storageConfig:
      boltdb_shipper:
        active_index_directory: /var/loki/index
        cache_location: /var/loki/index_cache
        resync_interval: 5s
        shared_store: s3
      aws:
        s3: http://console:console123@k8s-monitoring-minio:9000/loki-store
        s3forcepathstyle: true

tempo:
  nameOverride: "k8s-monitoring-tempo"
  tempo:
    metricsGenerator:
      enabled: true
      remoteWriteUrl: "http://k8s-monitoring-mimir-nginx.monitoring.svc:80/api/v1/push"
    storage:
      trace:
        backend: s3
        s3:
          bucket: tempo-store                        # store traces in this bucket
          endpoint: "k8s-monitoring-minio:9000"  # api endpoint
          access_key: console                                 # optional. access key when using static credentials.
          secret_key: console123                             # optional. secret key when using static credentials.
          insecure: true
          forcepathstyle: true                                 # optional. enable if endpoint is http\

opentelemetry-collector:
  # Valid values are "daemonset", "deployment", and "statefulset".
  mode: "deployment"
  presets:
    kubernetesEvents:
      enabled: true
  config:
    exporters:
      debug:
        verbosity: detailed
      otlphttp:
        endpoint: http://k8s-monitoring-mimir-nginx.monitoring.svc:80/otlp
    processors:
      batch: {}
      memory_limiter:
        check_interval: 5s
        limit_percentage: 80
        spike_limit_percentage: 25
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: ${env:MY_POD_IP}:4317
          http:
            endpoint: ${env:MY_POD_IP}:4318
      prometheus:
        config:
          scrape_configs:
            - job_name: opentelemetry-collector
              scrape_interval: 10s
              static_configs:
                - targets:
                    - ${env:MY_POD_IP}:8888
    service:
      telemetry:
        metrics:
          address: ${env:MY_POD_IP}:8888
      extensions:
        - health_check
      pipelines:
        logs:
          exporters:
            - debug
          processors:
            - memory_limiter
            - batch
          receivers:
            - otlp
        metrics:
          exporters:
            - debug
            - otlphttp
          processors:
            - memory_limiter
            - batch
          receivers:
            - otlp
            - prometheus
        traces:
          exporters:
            - debug
          processors:
            - memory_limiter
            - batch
          receivers:
            - otlp

  image:
    # If you want to use the core image `otel/opentelemetry-collector`, you also need to change `command.name` value to `otelcol`.
    repository: "otel/opentelemetry-collector-k8s"